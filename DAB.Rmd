---
title: "Data Analysis for Business Project"
author: "Michele Turco"
date: "2024-04-09"
output: html_document
---

# Introduction

The dataset contains information about bank customers. The goal is to predict whether a customer will close his credit card account, in that case the bank may try to propose more convenient services and prevent the client from leaving the bank.

# Setup

In order to run all the chunks below, the following libraries need to be imported. However, for the sake of simplicity, the code is not displayed in the generated pdf file.

```{r}
library(ggplot2)
library(corrplot)
library(class)

```

# Data Import

```{r}
Data = read.csv2("./Dataset/bank_accounts_train.csv", 
                 header = T, 
                 sep = ",", 
                 colClasses = "character")

```

# Data Preprocessing

In first place, we drop the first line since it is a pure identifier and it has no impact on the goal of the analysis. Then, we describe the variables present in the dataset.

```{r}

Data$CLIENTNUM = NULL 
str(Data)

```

From the output, we infer that we have 15 numerical variables, 4 categorical variables and our target variable "Closed_Account".

We proceed by turning numerical variables into numeric values and categorical variables into factors.

```{r}
variables = colnames(Data) # With this line we have a vector of our variables
categorical_variables <- c("Gender", "Education_Level", "Marital_Status", "Card_Category") # We select our categorical variables (only 4 of them)
target_variable = "Closed_Account"
numerical_variables <- setdiff(variables, c(categorical_variables, target_variable)) 
for (var in numerical_variables) {
  Data[[var]] = as.numeric(Data[[var]])
}

for (var in categorical_variables) {
  Data[[var]] = as.factor(Data[[var]])
}

Data$Closed_Account = as.factor(Data$Closed_Account)

str(Data)
Data_n = Data[numerical_variables]
```

# EDA

## Categorical Variables exploration

### Gender
```{r}
x = table(Data$Gender)
r = round(x/nrow(Data)*100, 2)
s = paste( r, "%", sep = "")
{pie(x = table(Data$Gender), 
     labels = s,
     edges = 10000, 
     radius = 1,
     init.angle = 90, 
     col = c(rgb(1,0,0, .5),
             rgb(0,0,1,0.5)),
     cex = 2)
  mtext("Gender", side = 3, cex = 2)
  legend("topright", 
         pch = 15, 
         col = c(rgb(1,0,0, .5),
                 rgb(0,0,1,0.5)),
         c("Female", "Male"), cex = 1.7,
         bty = "n")}

#For Gender we analyze some relations 

##Gender-Income
ggplot(Data, aes(x = Gender, y = Income, color = Gender)) +
  geom_boxplot() +
  xlab("Gender") +
  ylab("Income") +
  scale_color_manual(values = c("F" = "pink", "M" = "blue"))

##Gender-Customer_Age
ggplot(Data, aes(x = Gender, y = Customer_Age, color = Gender)) +
  geom_boxplot() +
  xlab("Gender") +
  ylab("Customer_Age") +
  scale_color_manual(values = c("F" = "pink", "M" = "blue"))

##Gender-Months_On_Book
ggplot(Data, aes(x = Gender, y = Months_on_book, fill = Gender)) +
  geom_violin(trim = FALSE, color = "black", alpha = 0.8) +  # Add black outline and reduce transparency
  scale_fill_manual(values = c("#FF9999", "#66CCFF", "#FFFF99", "#99FF99")) +  # Custom fill colors
  labs(
    title = "Distribution of Months on Book by Marital Status",
    x = "Marital Status",
    y = "Months on Book"
  )

##Gender-Closed_Account
#Let's adjust the dimensions of the legend
smaller_text_theme <- theme_minimal() +
  theme(legend.position = "bottom",
        legend.text = element_text(size = 8),
        axis.text = element_text(size = 8),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12),
        strip.text.x = element_text(size = 10))

ggplot(Data, aes(x = Gender, fill = as.factor(Closed_Account))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("0" = "lightgreen", "1" = "#FFFF99"), labels = c("Account Open", "Account Closed"), name = "") +
  labs(title = "Proportion of Closed Account By Gender", x = "Gender", y = "Proportion") +
  smaller_text_theme
```
### Marital Status
```{r}
#MARITAL STATUS
x <- table(addNA(Data$Marital_Status))
r = round(x/nrow(Data)*100, 2)
s = paste( r, "%", sep = "")
{pie(x = table(Data$Marital_Status), 
     labels = s,
     edges = 10000, 
     radius = 1,
     init.angle = 90, 
     col = c(rgb(1,0,0,0.5),
             rgb(0,0,1,0.5),
             rgb(0,0,0.5,1)
     ),
     cex = 1)
  mtext("Marital Status", side = 3, cex = 1.5, line = 1)
  legend("topright", 
         pch = 15, 
         col = c(rgb(1,0,0, .5),
                 rgb(0,0,1,0.5),
                 rgb(0,0, .5,1)
         ),
         c("Married", "Single", "Divorced"), cex = 1,
         bty = "n")}

#For Marital_Status we analyze some relations with numerical variables

##Marital_Status-Months_On_Book
ggplot(Data, aes(x = Marital_Status, y = Months_on_book, fill = Marital_Status)) +
  geom_violin(trim = FALSE, color = "black", alpha = 0.8) +  # Add black outline and reduce transparency
  scale_fill_manual(values = c("#FF9999", "#66CCFF", "#FFFF99", "#99FF99")) +  # Custom fill colors
  labs(
    title = "Distribution of Months on Book by Marital Status",
    x = "Marital Status",
    y = "Months on Book"
  )

##Marital_Status-Credit_Limit
ggplot(Data, aes(x = Marital_Status, y = Credit_Limit, fill = Marital_Status)) +
  geom_bar(stat = "identity", width = 0.7) +  # Set width of the bars
  labs(
    title = "Bar Plot Example",  # Title of the plot
    x = "Marital_Status",  # Label for the x-axis
    y = "Credit_Limit"  # Label for the y-axis
  ) +
  theme_minimal() +  # Use minimal theme
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis labels for better readability
  ) +
  scale_fill_manual(values = c("lightpink", "lightblue", "#FFFF99", "lightgreen"))  # Custom fill colors

##Marital_Status-Closed_Account

ggplot(Data, aes(x = Marital_Status, fill = as.factor(Closed_Account))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("0" = "lightgreen", "1" = "#FFFF99"), labels = c("Account Open", "Account Closed"), name = "") +
  labs(title = "Proportion of Closed Accounts by Marital_Status", x = "Income Bin", y = "Proportion") +
  smaller_text_theme
```
### Education Level

```{r}
#EDUCATION LEVEL
education_table <- table(Data$Education_Level)
r <- round(education_table / nrow(Data) * 100, 2)
s <- paste(r, "%", sep = "")

pie(x = education_table, 
    labels = s,
    edges = 10000, 
    radius = 1,
    init.angle = 90, 
    col = c(rgb(1,0,0,0.5),
            rgb(0,0,1,0.5),
            rgb(0,0,0.5,1),
            rgb(0.5,0.5,0,1),
            rgb(0.3,0,0.5,0.8),
            rgb(1,0.8,0,0.5)),
    cex = 1)
mtext("Education Level", side = 3, cex = 1.5, line = 1) # side=3 is the top
legend("topleft", 
       pch = 15, 
       col = c(rgb(1,0,0,0.5),
               rgb(0,0,1,0.5),
               rgb(0,0,0.5,1),
               rgb(0.5,0.5,0,1),
               rgb(0.3,0,0.5,0.8),
               rgb(1,0.8,0,0.5)),
       legend = levels(Data$Education_Level), cex = 1,
       bty = "n")

#For Education_Level we analyze some relations with numerical variables

##Education_Level-Credit_Limit
ggplot(Data, aes(x = Education_Level, y = Credit_Limit, fill = Education_Level)) +
  geom_bar(stat = "identity", width = 0.7) +  # Set width of the bars
  labs(
    title = "Credit Limit for each Educational Level",  # Title of the plot
    x = "Education_Level",  # Label for the x-axis
    y = "Credit_Limit"  # Label for the y-axis
  ) +
  theme_minimal() +  # Use minimal theme
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis labels for better readability
  ) +
  scale_fill_manual(values = c("lightpink", "lightblue", "#FFFF99", "lightgreen", "brown", "violet", "blue"))

##Education_Level-Closed_Account

ggplot(Data, aes(x = Education_Level, fill = as.factor(Closed_Account))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("0" = "lightgreen", "1" = "#FFFF99"), labels = c("Account Open", "Account Closed"), name = "") +
  labs(title = "Proportion of Closed Accounts by Education_Level", x = "Education_Level", y = "Proportion") +
  smaller_text_theme
```
### Card Category
```{r}

# Card Category
card_table <- table(Data$Card_Category)
r <- round(card_table / nrow(Data) * 100, 2)
s <- paste(r, "%", sep = "")
s2 <- paste(levels(Data$Card_Category), s)

pie(x = card_table, 
    edges = 10000, 
    radius = 1,
    init.angle = 90, 
    col = c(rgb(1,0,0,0.5),
            rgb(0,0,1,0.5),
            rgb(0,0,0.5,1),
            rgb(0.5,0.5,0,1)
    ),
    cex = 1)
mtext("Card Category", side = 3, cex = 1.5, line = 1) # side=3 is the top
legend( x= -2.4, y = 1,
        pch = 15, 
        col = c(rgb(1,0,0,0.5),
                rgb(0,0,1,0.5),
                rgb(0,0,0.5,1),
                rgb(0.5,0.5,0,1)
        ),
        legend = s2, cex = 1,
        bty = "n")

#For Card_Category we analyze some relations with numerical variables

##Card_Category-Customer_Age
ggplot(Data, aes(x = Card_Category, fill = as.factor(Closed_Account))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("0" = "lightgreen", "1" = "#FFFF99"), labels = c("Account Open", "Account Closed"), name = "") +
  labs(title = "Proportion of Closed Accounts by Card_Category", x = "Card_Category", y = "Proportion") +
  smaller_text_theme

```
### Graphical Exploration of Numerical Variables

```{r}

Numerical_Data = Data[numerical_variables]
means_vec = apply(X = Numerical_Data, MARGIN = 2, FUN = mean)
median_vec = apply(X = Numerical_Data, MARGIN = 2, FUN = median)
sd_vec = apply(X = Numerical_Data, MARGIN = 2, FUN = sd)

par(mfrow = c(3,3), mar = c(2,4,4,1))

for(i in 1:length(numerical_variables)){
  hist(Numerical_Data[,i], freq = F, main = names(Numerical_Data)[i],
       col = rgb(.7,.7,.7), border = "white", xlab = "")
  abline(v = means_vec[i], lwd = 2)
  abline(v = median_vec[i], lwd = 2, col = rgb(.7,0,0))
  legend("top", c("Mean", "Median"), lwd = 2, col = c(1, rgb(.7,0,0)),cex = .8, bty = "n")
}

```

As we can see from the density distributions, almost all the numerical variables have the mean and the median are close together, and hence their distribution is likely symmetric, as showed in the plots. Indeed, only few of them presents a long tail and are highly skewed to the left or the the right (Credit_Limit, Avg_Open_To_Buy and Avg_Utilization_Ratio) and, even in those cases, median and mean are still really close with respect to each other.

### Correlation Matrix

In the following chunk, we analyzed the correlation between numerical variables plotting a correlation matrix.

```{r}


cor_matrix <- cor(Data[numerical_variables], use="complete.obs")
corrplot(cor_matrix, method = "color", tl.srt = 45, tl.col = "black") 



```

From the correlation matrix we can clearly see some relevant correlations that make us think of a possible deletion. In particular we are searching for variables that are highly correlated so that we can discard one of them in order to avoid multicollinearity. It is worth to considering the relation between "Months_on_Book" and "Customer_Age","Total_Trans_Ct" and "Total_Trans_Amt", "Credit_Limit" and "Avg_Open_to_Buy". The most relevant one is the relation between "Credit_Limit" and "Avg_Open_to_Buy".However, we will just take into account these findings without any further modification to the original dataset.

# Dealing with unknown values

```{r}

#Rename missing values with NA notation
Data[Data == 'Unknown'] <- NA

# Now we try to understand where the missing values are

# Sum the TRUE values by row to see how many NAs each row contains
na_counts_per_row <- rowSums(is.na(Data))

# Count how many rows have at least one NA
rows_with_na <- sum(na_counts_per_row > 0)

# Print the result
print(rows_with_na)

# Since there is a significant number of rows with missing values, dropping them is not an
# optimal solution

# Count NAs in each column
na_counts_per_column <- colSums(is.na(Data))

# Print the result
print(na_counts_per_column)

# Missing values are only present in "Marital Status" and "Educational Level". 


#We decided to impute missing values with the mode of the other instances
#in the same column

getSimpleMode <- function(x) {
  # Use table() to count occurrences of each value, sort in decreasing order, and return the name of the first element
  tbl <- table(x)
  mode_value <- names(tbl[tbl == max(tbl)])[1]
  return(mode_value)
}


for (var in categorical_variables) {
  if (any(is.na(Data[[var]]))) {
    Data[[var]][is.na(Data[[var]])] <- getSimpleMode(Data[[var]])
  }
}

# Sum the TRUE values by row to see how many NAs each row contains
na_counts_per_row <- rowSums(is.na(Data))

# Count how many rows have at least one NA
rows_with_na <- sum(na_counts_per_row > 0)

# Print the result
print(rows_with_na)

str(Data)

```

# 3 Logistic Regression Model to estimate effects of Income and Gender on Closed_Account

In the first place, we fit the model to the data and we consider the summary of the model. In this case, we do not exclude the hypothesis that Income and Gender interact with each other and consequently, the predictors used are both of them and their interaction.

```{r}

# Fit logistic regression model
set.seed(1)
gender_income_model <- glm(Closed_Account ~ Income * Gender, family = binomial(link = "logit"), data = Data)

# View model summary
summary(gender_income_model)

```

Intercept: The estimated probability of an account being closed when Income is 0 and Gender is Female is -1.5474767. This is statistically significant with a very low p-value (< 2e-16), indicating strong evidence against the null hypothesis of no effect.This is something that we could have expected considering the highly unbalanced distribution of the target variable, as stated in the EDA process.

Income: The coefficient for Income is -0.0001008 and is not statistically significant (p-value 0.89207), suggesting that this variables does not have a strong effect on the probability of account closure.

We then proceed by computing predictions and plotting the respective regression lines:

```{r}

predicted_probabilities_logistic <- predict(gender_income_model, newdata = Data, type="response")

Data$predicted_probabilities <- predict(gender_income_model, newdata = Data, type="response")
```

```{r}
#Plotting computed probabilities
ggplot(Data, aes(x = Income, y = predicted_probabilities, color = Gender)) + 
  geom_line() + 
  labs(title = "Probability of Account Closure by Income and Gender", y = "Probability of Closure", x = "Income") +
  scale_color_manual(values = c("red", "blue"))

Data$predicted_probabilities = NULL # We drop the column of predicted probabilities since it is now useless


```



# 4 KNN
The first step required is the split of the training data between validation and train set. After the splitting is performed, there is the need to check whether the distribution of the target variable remained constant in the validation and in the test set.


```{r}
set.seed(123)
id_train <- sample(1:nrow(Data), size = 0.75*nrow(Data), replace = F)
train_data <- Data[id_train,]
val_data <- Data[-id_train,]

# Response variable distribution in the original data
print("Distribution of the target variable in the original set")
table(Data$Closed_Account)
prop.table(table(Data$Closed_Account))

# Response variable distribution in the train test
print("Distribution of the target variable in the train set")
table(train_data$Closed_Account)
prop.table(table(train_data$Closed_Account))



# Response variable distribution in the validation set
print("Distribution of the target variable in the validation set")
table(val_data$Closed_Account)
prop.table(table(val_data$Closed_Account))
```

In this case, the distribution is really similar between different sets.

KNN is a model that requires numerical 
```{r}
# Set a range for k
k_values <- 1:25
accuracy_scores <- numeric(length(k_values))

# Loop over k values
for (k in k_values) {
  set.seed(123) # for reproducibility
  knn_pred <- knn(train = train_data[, c("Total_Trans_Amt", "Total_Trans_Ct")],
                  test = val_data[, c("Total_Trans_Amt", "Total_Trans_Ct")],
                  cl = train_data$Closed_Account,
                  k = k)
  
  # Calculate accuracy
  accuracy_scores[k] <- sum(val_data$Closed_Account == knn_pred) / length(knn_pred)
}

# Now, plot the accuracy scores as a function of k
plot(k_values, accuracy_scores, type = "b", 
     xlab = "Number of Neighbors (k)", ylab = "Accuracy",
     main = "k-NN Model Accuracy by Number of Neighbors")
```

# 5 Best Model Selection

For the sake of completeness, we repeat the process of splitting the data into train and validation set

```{r}

set.seed(10)
id_train <- sample(1:nrow(Data), size = 0.75*nrow(Data), replace = F)
train_data <- Data[id_train,]
val_data <- Data[-id_train,]



```

In the first place, we consider as baseline the models in which no variable is used and all variables are included. Firstly, we build the model with each variable:

```{r}
logit_fit_baseline <- glm(Closed_Account ~ .,
                  family = "binomial",
                  data = train_data)
summary(logit_fit_baseline)

```

Then, we consider the model without any variable:

```{r}
logit_fit_0 <- glm(Closed_Account ~ 1,
                  family = "binomial",
                  data = train_data)
summary(logit_fit_0)

```

We then test the hypothesis of equivalence between the two models:

```{r}
logit_fit0 <- glm(Closed_Account ~ 1,
                  family = "binomial",
                  data = train_data)
anova(logit_fit_0, logit_fit_baseline, test = "Chisq") 

```

The difference in deviance between the two models is 2233.5 with 26 degrees of freedom, which is highly significant (p < 0.00000000000000022). This indicates that the full model provides a significantly better fit to the data than the null model.

## Variable Selection with AIC and BIC

In spite of the result obtained before, there are too many covariates, some of which are not really significant for the model. We can then perform some type of variable selection to keep only the most important ones.


### Stepwise variable selection (based on AIC) 

Forward selection: 
```{r}
# Forward
logit_fit_aic1 <- step(glm(Closed_Account ~ 1,
                           family = "binomial",
                           data = train_data),
                       scope = formula(logit_fit_baseline),
                       direction = "forward")
```

The variable excluded by the forward selection, which obtains the best (least value) AIC=2880.6, are the following:
Income, Customer_Age, Months_on_book, Avg_Open_To_Buy, Credit_Limit, Avg_Utilization_Ratio, Card_Category and Gender.

Backward selection:
```{r}
# Backward
logit_fit_aic2 <- step(logit_fit_baseline,
                       direction = "backward") 

```

The variable excluded by the backward selection, which obtains the best (least value) AIC=2880.5: are the following:
Gender, Months_on_book, Credit Limit, Avg_Open_To_Buy, Avg_Utilization_Ratio and Customer Age, Card Category and Income.

Both directions:
```{r}
# Both directions
logit_fit_aic3 <- step(logit_fit_baseline,
                       direction = "both")
```

The variable excluded by the selection in both directions, which obtains the best (least value) AIC=2794.9: are the following:
Months_on_book, Avg_Open_To_Bu and Avg_Utilization_Ratio, Credit Limit, 

In order to select the best model, we identify the one with the lowest number of covariates:
```{r}

print(length(coefficients(logit_fit_aic1)))
print(length(coefficients(logit_fit_aic2)))
print(length(coefficients(logit_fit_aic3)))

```

### Stepwise variable selection (based on BIC) 

Forward selection: 
```{r}

# Forward
logit_fit_bic1 <- step(glm(Closed_Account ~ 1,
                           family = "binomial",
                           data = train_data),
                       scope = formula(logit_fit_baseline),
                       direction = "forward",
                       k = log(nrow(train_data)))
```

The variable excluded by the forward selection, which obtains the best (least value) AIC=2794.92, are the following:
Card_Category, Customer_Age, Months_on_book, Avg_Open_To_Bu and Avg_Utilization_Ratio.

Backward selection:
```{r}
# Backward
logit_fit_bic2 <- step(logit_fit_baseline,
                       direction = "backward",
                       k = log(nrow(train_data)))  

```

The variable excluded by the backward selection, which obtains the best (least value) AIC=2794.54: are the following:
Educational_Level, Card Category, Months_on_book, Credit_Limit, Avg_Utilization_Ratio, Avg_Open_To_Buy, Customer_Age and Income.

Both directions:
```{r}
# Both directions
logit_fit_bic3 <- step(logit_fit_baseline,
                       direction = "both",
                       k = log(nrow(train_data)))
```

The variable excluded by the selection in both directions, which obtains the best (least value) AIC=2794.9: are the following:
Income, Customer_Age, Months_on_book, Avg_Open_To_Buy, Credit_Limit, Avg_Utilization_Ratio, Card_Category and Educational_Level.

In order to select the best model, we identify the one with the lowest number of covariates:
```{r}

print(length(coefficients(logit_fit_bic1)))
print(length(coefficients(logit_fit_bic2)))
print(length(coefficients(logit_fit_bic3)))

```

## PCA 

In order to perform a PCA, we have to consider only numerical variables in the train data. Then, considering the magnitude of the covariance matrix, a scaling process is needed.
```{r}

train_data_Numerical = train_data[numerical_variables]

cov(train_data_Numerical)

train_data_Numerical_Scaled= scale(train_data_Numerical)

cov(train_data_Numerical_Scaled)

```

```{r}

pca <- princomp(train_data_Numerical_Scaled, cor = T, scale = F) #already scaled

pca$loadings

# Calculating the variance (in percentage) explained by each PCA component
pca_var <- pca$sdev^2
pca_var_percent <- pca_var / sum(pca_var)

```

```{r}
library(RColorBrewer)

bar.comp = barplot(pca_var_percent,
                   las = 2,
                   col = rev(brewer.pal(9, "Blues")), 
                   border = F,
                   ylim = c(0, max(pca_var_percent)*1.5),
                   ylab = "Explained variance")

# choose components according to "elbow rule"

lines(x = bar.comp, pca_var_percent, type = "b", pch = 16, cex = 1.5, lwd = 2, col = 2)
grid()

```

```{r}

# Calculating the cumulative variance explained
cum_pca_var_percent <- cumsum(pca_var_percent)*100

# Plotting the cumulative distribution

bar.comp = barplot(cum_pca_var_percent,
                   las = 2,
                   col = rev(brewer.pal(9, "Blues")), 
                   border = F,
                   ylim = c(0, 105),
                   ylab = "Explained variance")

lines(bar.comp, cum_pca_var_percent, type = "b", pch = 16, cex = 1.5, lwd = 2, col = 2)
grid()

```

Considering the barplot, it is clear that the total variance is mostly explained by the first 11 components that are consequently selected. The first component is the one with the highest percentage of explained variance, with a value of 0.17.

```{r}

val_data_Numerical = val_data[numerical_variables] # Validation data is scaled as the train data
val_data_Numerical_Scaled= scale(val_data_Numerical)

# Predicting scores for the validation set using the PCA model
pca_data <- predict(pca, train_data_Numerical_Scaled)

# Selecting the first 11 principal components
pca_data <- pca_data[, 1:11]


logit_pca = glm(Closed_Account ~ ., data = data.frame(pca_data, Closed_Account = train_data$Closed_Account), 
                family = "binomial")
summary(logit_pca)

```

## Best model selection

Once we have selected the best model with AIC, BIC and PCA, we are now able to test them on the validation set in order to understand which of them performs in the best way.
```{r}

```
